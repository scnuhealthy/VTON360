seed: 42

model_path: "/GPUFS/sysu_gbli2_1/hzj/pretrained_models/stable-diffusion-v1-5"
vae_path: "/GPUFS/sysu_gbli2_1/hzj/pretrained_models/sd-vae-ft-mse"
clip_model_path: '/GPUFS/sysu_gbli2_1/hzj/pretrained_models/clip-vit-base-patch32'

# unet_path: "thuman_tryon_exp_0807/checkpoint-140000"
# pretrained_poseguider_path: "thuman_tryon_exp_0807/checkpoint-140000/pose.ckpt"
# pretrained_referencenet_path: 'thuman_tryon_exp_0807/checkpoint-140000'

# unet_path: "mvhumannet_tryon_exp_multi_1028/checkpoint-24000"
# pretrained_poseguider_path: "mvhumannet_tryon_exp_multi_1028/checkpoint-24000/pose.ckpt"
# pretrained_referencenet_path: 'mvhumannet_tryon_exp_multi_1028/checkpoint-24000'

# unet_path: "checkpoints/thuman_tryon_exp_multi_1025/checkpoint-22000"
# pretrained_poseguider_path: "checkpoints/thuman_tryon_exp_multi_1025/checkpoint-22000/pose.ckpt"
# pretrained_referencenet_path: 'checkpoints/thuman_tryon_exp_multi_1025/checkpoint-22000'

unet_path: "checkpoints/mvhumannet_tryon_mvattn_multi_1205/checkpoint-40000"
pretrained_poseguider_path: "checkpoints/mvhumannet_tryon_mvattn_multi_1205/checkpoint-40000/pose.ckpt"
pretrained_referencenet_path: 'checkpoints/mvhumannet_tryon_mvattn_multi_1205/checkpoint-40000'

# unet_path: "thuman_tryon_exp_1015_two/checkpoint-60000"
# pretrained_poseguider_path: "thuman_tryon_exp_1015_two/checkpoint-60000/pose.ckpt"
# pretrained_referencenet_path: 'thuman_tryon_exp_1015_two/checkpoint-60000'

out_dir: 'image_test_w_mvhumannet2_multi_22000_mubs_111'
# out_dir: 'image_output_tryon_mvhumannet_1028_24000_test_multi_2'
# out_dir: 'image_output_tryon_mvhumannet_1028_14000_all_multi_cfg2_back'

batch_size: 2
dataloader_num_workers: 4
guidance_scale: 2  # thuman:3 mvhumannet:2


# infer_data:
#   # dataroot: "/GPUFS/sysu_gbli2_1/hzj/render_data"
#   dataroot: "/GPUFS/sysu_gbli2_1/hzj/save_render_data_yw/"
#   # sample_size:  [512,384] # for 40G 256
#   sample_size:  [768,576]
#   clip_model_path: '/GPUFS/sysu_gbli2_1/hzj/pretrained_models/clip-vit-base-patch32'
#   is_train: false
#   mode: 'pair'

infer_data:
  # dataroot: "/GPUFS/sysu_gbli2_1/hzj/render_data"
  dataroot: "/GPUFS/sysu_gbli2_1/hzj/mvhumannet/"
  # sample_size:  [512,384] # for 40G 256
  sample_size:  [768,576]
  clip_model_path: '/GPUFS/sysu_gbli2_1/hzj/pretrained_models/clip-vit-base-patch32'
  is_train: false
  mode: 'pair'

fusion_blocks: "full"
image_finetune: true
num_inference_steps: 30